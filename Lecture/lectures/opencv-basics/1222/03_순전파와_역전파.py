# 순전파와 역전파

# 쉽게 이해하기:
# 학습 과정 = 예측하고 → 틀린 정도 확인하고 → 고치기를 반복
# - 순전파: 예측하기
# - 역전파: 무엇을 고쳐야 할지 찾기


# 순전파 (Forward Propagation)
# = 입력 -> 출력 방향으로 계산 진행

# 쉬운 비유:
# 공장 생산 라인처럼 앞으로 쭉 진행
# [원료] → [1단계 가공] → [2단계 조립] → [3단계 포장] → [완제품]

# 과정:
# 1. 입력 데이터 받기
# 2. 각 층에서 가중합 계산: z = Wx + b
# 3. 활성화 함수 적용: a = f(z)
# 4. 다음 층으로 전달
# 5. 출력층까지 반복

# 2층 신경망 예시:
# 층1: z₁ = W₁x + b₁  (가중합 계산)
#      a₁ = f(z₁)     (활성화 함수 적용)

# 층2: z₂ = W₂a₁ + b₂ (가중합 계산)
#      a₂ = f(z₂)     (활성화 함수 적용)

# 출력: ŷ = a₂

# 구체적 예시:
# x = 2.0 (입력)
# w = 0.5 (가중치)
# b = 1.0 (편향)
# y = 3.0 (실제 정답)

# 순전파 계산:
# 1. z = w × x + b
#      = 0.5 × 2.0 + 1.0
#      = 2.0

# 2. ŷ = σ(z)  (σ는 활성화 함수, 예: sigmoid)
#      = 1 / (1 + e^(-2.0))
#      ≈ 0.88

# 3. 손실 계산: L = (ŷ - y)²  (예측과 정답의 차이)
#      = (0.88 - 3.0)²
#      = (-2.12)²
#      ≈ 4.49

# 결과: 예측 0.88, 정답 3.0 → 많이 틀렸네요!


# 역전파 (Backpropagation)

# 쉬운 비유:
# 불량품이 나왔을 때 원인 찾기
# [완제품] ← "불량품이네!" (오차 발견)
# [3단계] ← "포장 문제?" (확인)
# [2단계] ← "조립 문제?" (확인)
# [1단계] ← "아! 가공이 문제구나!" (원인 발견)

# 정의:
# 정답과 오차(손실)가 줄어들도록 가중치w, 편향b를 어떻게 바꿀지를
# 미분(체인룰)로 계산해서, 각 층에 '너는 얼마나 책임이 있니?'를 나눠주는 방법

# 왜 '거꾸로' 가냐?
# - 출력 오차는 출력층 파라미터와 가장 직접적으로 연결
# - 그 오차가 그 전 층의 출력 때문에 생긴 부분도 있고
# - 더 전 층 때문에 생긴 부분도 있음
# → 그래서 '오차를 만든 책임'을 출력층에서 시작해 이전 층으로 분배!
# → 이때 쓰는 게 체인 룰(연쇄 법칙)


# 연쇄 법칙 (Chain Rule)
# = 함수가 중첩되어 있을 때 미분하는 방법

# 쉬운 비유:
# 도미노 효과 - 앞의 변화가 뒤에 어떤 영향을 미치는지 계산

# 기본 공식:
# y = f(g(x)) 일 때
# dy/dx = (dy/dg) × (dg/dx)
# 읽는 법: "y를 x로 미분 = y를 g로 미분 × g를 x로 미분"

# 예시: y = (2x + 3)²

# 방법 1: 직접 전개해서 미분
# y = 4x² + 12x + 9
# dy/dx = 8x + 12

# 방법 2: 연쇄 법칙 (중간 변수 사용)
# u = 2x + 3  →  du/dx = 2
# y = u²      →  dy/du = 2u

# 연쇄 법칙 적용:
# dy/dx = (dy/du) × (du/dx)
#       = 2u × 2
#       = 2(2x + 3) × 2
#       = 4(2x + 3)
#       = 8x + 12  ← 같은 결과!

# x = 1일 때:
# dy/dx = 8(1) + 12 = 20

# 왜 연쇄 법칙을 쓰는가?
# 신경망은 함수의 중첩 구조:
# 출력 = f₅(f₄(f₃(f₂(f₁(입력)))))

# 직접 미분? → 너무 복잡!
# 연쇄 법칙? → 가능! (단계별로 곱하면 됨)

# ∂출력/∂입력 = (∂출력/∂f₅) × (∂f₅/∂f₄) × (∂f₄/∂f₃) × (∂f₃/∂f₂) × (∂f₂/∂f₁) × (∂f₁/∂입력)


# 역전파 예시 (가장 단순한 경우)

# 시나리오:
# x → [w곱하기] → z → [제곱] → L

# 수식:
# z = w × x  (w와 x를 곱함)
# L = z²     (z를 제곱함)

# 목표: ∂L/∂w 구하기 (L을 w로 미분 = "w가 L에 얼마나 영향을 주는가?")

# 구체적 숫자:
# x = 3
# w = 2

# 순전파:
# z = w × x = 2 × 3 = 6
# L = z² = 36

# 역전파 (연쇄 법칙 적용):
# ∂L/∂w = (∂L/∂z) × (∂z/∂w)

# 1. ∂L/∂z = 2z = 2 × 6 = 12
#    (L = z²를 z로 미분)

# 2. ∂z/∂w = x = 3
#    (z = w × x를 w로 미분)

# 3. ∂L/∂w = 12 × 3 = 36

# 해석:
# w가 1 증가하면 → L이 약 36 증가
# 따라서 w를 줄여야 L이 감소!


# 경사하강법 (Gradient Descent)
# = 오차를 줄이기 위해 파라미터를 업데이트하는 방법

# 쉬운 비유:
# 안개 낀 산에서 내려오기
# - 눈 가리고 산 정상에 있음
# - 발로 주변 더듬어서 아래 방향 찾기
# - 그 방향으로 한 걸음씩 내려가기

# 공식:
# w_new = w_old - η × (∂L/∂w)
# η (eta) = 학습률 (한 걸음의 크기)

# 예시 (위의 계산 이어서):
# η = 0.01 (학습률)
# w_old = 2
# ∂L/∂w = 36

# w_new = 2 - 0.01 × 36 = 1.64

# 검증 (순전파로 다시 계산):
# z = w × x = 1.64 × 3 = 4.92
# L = z² = 4.92² = 24.21

# 결과: 손실이 36 → 24.21로 감소! ✓


# 학습률 (Learning Rate)의 중요성

# 학습률이 너무 크면 (예: η = 1.0):
# - 한 걸음이 너무 커서 최적점을 지나칠 수 있음
# - 학습이 불안정

# 학습률이 너무 작으면 (예: η = 0.0001):
# - 한 걸음이 너무 작아서 학습이 너무 느림
# - 시간이 오래 걸림

# 적절한 학습률 (예: η = 0.001 ~ 0.01):
# - 안정적으로 빠르게 학습


# 전체 학습 과정 요약

# 1. 순전파: 입력 → 예측값 계산
# 2. 손실 계산: 예측값과 정답의 차이
# 3. 역전파: 각 파라미터가 손실에 미친 영향 계산 (미분)
# 4. 경사하강법: 파라미터 업데이트
# 5. 반복!

# 비유로 기억하기:
# - 순전파 = 공장 생산 라인 (앞으로 진행)
# - 역전파 = 불량품 원인 찾기 (거꾸로 추적)
# - 경사하강법 = 산 내려가기 (오차 줄이기)
