# 인공 신경망의 구조

# 쉽게 이해하기:
# 우리 뇌는 어떻게 작동할까?
# 눈으로 사과를 보면 → 뇌의 뉴런들이 신호를 주고받으며 → "사과다!" 인식
# 인공 신경망도 이 과정을 모방합니다!


# 생물학적 뉴런 vs 인공 뉴런

# 생물학적 뉴런
# 구조:
# 수상돌기 -> 세포체 -> 축삭 -> 시냅스

# 동작:
# - 수상돌기: 다른 뉴런에서 신호 수신
# - 세포체: 신호 합산 및 처리
# - 축삭: 신호 전달
# - 시냅스: 다음 뉴런으로 신호 전송

# 특징:
# - 임계값 이상이면 발화(firing)
# - 약 860억 개의 뉴런
# - 100조 개의 시냅스 연결


# 인공 뉴런 (퍼셉트론)

# 쉬운 비유:
# 인공 뉴런 = 작은 계산기
# 여러 입력을 받아서 → 중요도에 따라 조절하고 → 결과를 내보냄

# 동작 과정:
# 입력 (x₁, x₂, ..., xₙ) ← 특징들
#     ↓
# 가중치 곱 (w₁x₁ + w₂x₂ + ... + wₙxₙ) ← 입력의 중요도 (시냅스 강도처럼)
#     ↓
# 편향 더하기 (+b) ← 기준점 조절 (임계값을 옮기는 효과)
#     ↓
# 활성화 함수 f( ) ← '얼마나 켤지' 결정
#     ↓
# 출력 y

# 수식: y = f(Σwᵢxᵢ + b)
# 읽는 법: "모든 입력에 가중치를 곱해서 더하고, 편향을 더한 뒤, 활성화 함수를 통과"

# 한 줄 요약:
# 생물학적 뉴런: 전기·화학 신호를 시간에 따라 통합하고, 임계치 넘으면
#                발화하며 연결 강도가 변하면서 학습

# 인공 뉴런: 입력을 가중합한 뒤 활성화 함수를 통과시켜 출력하고,
#            오차를 줄이도록 가중치를 업데이트하며 학습


# 신경망의 층 구조

# 쉬운 비유:
# 신경망 = 회사 조직도처럼 층층이 쌓인 구조
# [입력층] → [은닉층1] → [은닉층2] → [출력층]
#  데이터     특징추출     패턴인식     최종결정

# 층의 종류:

# 1. 입력층 (Input Layer)
# - 데이터를 받아들이는 층
# - 특성 개수 = 뉴런 개수
# - 계산 없음, 단순 전달
# 예) 이미지 28x28 픽셀 → 784개 뉴런

# 2. 은닉층 (Hidden Layer)
# - 입력과 출력 사이의 층
# - 특성 추출 및 변환 (실제 학습이 일어나는 곳!)
# - 1개 이상 (딥러닝 = 많은 은닉층)
# 예) 1층: 선 찾기, 2층: 모양 찾기, 3층: 물체 인식

# 3. 출력층 (Output Layer)
# - 최종 예측 결과
# - 작업에 따라 뉴런 개수 결정
# 예) 이진 분류 → 1개, 10개 클래스 분류 → 10개

import torch.nn as nn

model = nn.Sequential(
    nn.Linear(4, 3),  # 입력층 -> 은닉층1 (4->3)
    nn.ReLU(),        # 활성화
    nn.Linear(3, 3),  # 은닉층1 -> 은닉층2 (3->3)
    nn.ReLU(),        # 활성화
    nn.Linear(3, 2),  # 은닉층2 -> 출력층 (3->2)
)

print(model)


# 가중치와 편향

# 가중치 (Weight) = 연결의 강도

# 쉬운 비유:
# 가중치 = 음악 믹서의 볼륨 조절기
# - 보컬: 크게 (가중치 높음) → 중요!
# - 배경음: 작게 (가중치 낮음) → 덜 중요

# 역할:
# - 입력의 중요도 결정
# - 양수: 입력과 같은 방향 (긍정적 영향)
# - 음수: 입력과 반대 방향 (부정적 영향)
# - 0에 가까움: 영향 적음

# 학습 과정:
# - 초기: 무작위 초기화
# - 학습: 역전파로 업데이트
# 예) 처음에는 w=0.5였는데, 학습 후 w=2.3으로 변함


# 편향 (Bias) = 뉴런의 기준점 조절

# 쉬운 비유:
# 편향 = 시험 가산점
# - 60점이 합격 기준인데, +5점 가산점 주면?
# - 55점도 합격 가능! (기준이 낮아짐)

# 역할:
# - 활성화 임계값 조절
# - 입력이 0일 때도 출력 가능
# - 모델의 유연성 증가

# 수식 비교:
# y = wx (편향 없음) → 원점 통과 (x=0이면 y=0)
# y = wx + b (편향 있음) → y절편 = b (x=0이어도 y=b)


# 파라미터 개수 계산

# 파라미터 = 학습을 통해 조정되는 값 (가중치 + 편향)

# 공식:
# Linear 층의 파라미터 개수 = (입력 크기 × 출력 크기) + 출력 크기
#                           =     가중치 개수      + 편향 개수

# 예시 1: Linear(10, 5)
# - 가중치: 10 × 5 = 50개 (입력 10개와 출력 5개를 연결)
# - 편향: 5개 (각 출력 뉴런마다 1개)
# - 총: 50 + 5 = 55개

# 쉬운 이해:
# 입력 뉴런 10개 → 출력 뉴런 5개로 가려면?
# - 각 입력이 모든 출력과 연결 = 10 × 5 = 50개 연결
# - 각 출력마다 기준점(편향) 1개씩 = 5개
# - 합계 = 55개 파라미터

import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(4, 3),  # 입력층 -> 은닉층1 (4 -> 3) =>  4 × 3 + 3 = 15
    nn.ReLU(),        # 활성화
    nn.Linear(3, 3),  # 은닉층1 -> 은닉층2 (3 -> 3) => 3 × 3 + 3 = 12
    nn.ReLU(),        # 활성화
    nn.Linear(3, 2),  # 은닉층2 -> 출력층 (3 -> 2) => 3 × 2 + 2 = 8
)

# 총 파라미터 개수
total_params = sum(p.numel() for p in model.parameters())
print(f'총 파라미터: {total_params}')
print('(15 + 12 + 8 = 35개)')

# 층별 파라미터 상세
print('\n층별 파라미터:')
for name, param in model.named_parameters():
    print(f'{name}: {param.shape} → {param.numel()}개')

# 왜 파라미터 개수가 중요한가?
# - 파라미터가 많으면: 복잡한 패턴 학습 가능, 하지만 느리고 메모리 많이 씀
# - 파라미터가 적으면: 학습 빠름, 하지만 단순한 것만 학습 가능
