# 인공 신경망의 구조

# 생물학적 뉴런 vs 인공 뉴런
# 생물학적 뉴런
# 구조
# 수상돌기 -> 세포체 -> 축삭 -> 시냅스

# 동작
# 수상돌기: 다른 뉴런에서 신호 수신
# 세포체: 신호 합산 및 처리
# 축삭: 신호 전달
# 시냅스: 다음 뉴런으로 신호 전송

# 특징
# 임계값 이상이면 발화(firing)
# 약 860억 개의 뉴런
# 100조 개의 시냅스 연결


# 인공 뉴런 (퍼셉트론)
# 입력 (x₁, x₂, ..., xₙ) (특징)
#     ↓
# 가중치 곱 (w₁x₁ + w₂x₂ + ... + wₙxₙ) (입력의 중요도) => 시냅스 강도처럼
#     ↓
# 편향 더하기 (+b) (기준점 조절) => 임계값을 옮기는 효과
#     ↓
# 활성화 함수 f( ) '얼마나 켤지' 결정
#     ↓
# 출력 y

# 수식: y = f(Σwᵢxᵢ + b)

# 한 줄 요약
# 생물학적 뉴런: 전기·화학 신호를 시간에 따라 통합하고, 임계치 넘으면
# 발화하며 연결 강도가 변하면서 학습

# 인공 뉴런: 입력을 가중합한 뒤 활성화 함수를 통과시켜 출력하고,
# 오차를 줄이도록 가중치를 업데이트하며 학습

# 신경망의 층 구조
# 층의 종류

# 입력층 (Input Layer)
# 데이터를 받아들이는 층
# 특성 개수 = 뉴런 개수
# 계산 없음, 단순 전달

# 은닉층 (Hidden Layer)
# 입력과 출력 사이의 층
# 특성 추출 및 변환
# 1개 이상 (딥러닝 = 많은 은닉층)

# 출력층 (Output Layer)
# 최종 예측 결과
# 작업에 따라 뉴런 개수 결정

import torch.nn as nn

model = nn.Sequential(
    nn.Linear(4, 3),  # 입력층 -> 은닉층1 (4->3)
    nn.ReLU(),        # 활성화
    nn.Linear(3, 3),  # 은닉층1 -> 은닉층2 (3->3)
    nn.ReLU(),        # 활성화
    nn.Linear(3, 2),  # 은닉층2 -> 출력층 (3->2)
)

print(model)

# 가중치와 편향
# 가중치 = 연결의 강도

# 역할
# 입력의 중요도 결정
# 양수: 입력과 같은 방향
# 음수: 입력과 반대 방향
# 0에 가까움: 영향 적음

# 학습 대상:
# 초기: 무작위 초기화
# 학습: 역전파로 업데이트

# 편향
# 뉴런의 기준점 조절

# 역할
# 활성화 임계값 조절
# 입력이 0일 때도 출력 가능
# 유연성 증가

# 수식 비교
# y = wx (편향 없음) → 원점 통과
# y = wx + b (편향 있음) → y절편 = b

# 파라미터 개수 계산
# Linear 층의 파라미터 개수 = (입력 크기 x 출력 크기) + 출력 크기
#                           =       가중치 개수     + 편향 개수

# Linear(10, 5)
# 가중치: 10 x 5 = 50개 (입력 10개와 출력 5개를 연결)
# 편향: 5개 (각 출력 뉴런마다 1개)
# 총 50 + 5 = 55개

import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(4, 3),  # 입력층 -> 은닉층1 (4 -> 3) =>  4 x 3 + 3 = 15
    nn.ReLU(),        # 활성화
    nn.Linear(3, 3),  # 은닉층1 -> 은닉층2 (3 -> 3) => 3 x 3 + 3 = 12
    nn.ReLU(),        # 활성화
    nn.Linear(3, 2),  # 은닉층2 -> 출력층 (3 -> 2) => 3 x 2 + 2 = 8
)

# 총 파라미터 개수
total_params = sum(p.numel() for p in model.parameters())
print(f'총 파라미터: {total_params}')

# 층별 파라미터
for name, param in model.named_parameters():
    print(f'{name}: {param.shape}')
