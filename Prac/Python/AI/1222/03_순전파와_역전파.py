# 순전파와 역전파

# 쉽게 이해하기:
# 학습 과정 = 예측하고 → 틀린 정도 확인하고 → 고치기를 반복
# - 순전파: 예측하기
# - 역전파: 무엇을 고쳐야 할지 찾기


# 순전파 (Forward Propagation)
# = 입력 -> 출력 방향으로 계산 진행

# 쉬운 비유:
# 공장 생산 라인처럼 앞으로 쭉 진행
# [원료] → [1단계 가공] → [2단계 조립] → [3단계 포장] → [완제품]

# 과정:
# 1. 입력 데이터 받기
# 2. 각 층에서 가중합 계산: z = Wx + b
# 3. 활성화 함수 적용: a = f(z)
# 4. 다음 층으로 전달
# 5. 출력층까지 반복

# 2층 신경망 예시:
# 층1: z₁ = W₁x + b₁  (가중합 계산)
#      a₁ = f(z₁)     (활성화 함수 적용)

# 층2: z₂ = W₂a₁ + b₂ (가중합 계산)
#      a₂ = f(z₂)     (활성화 함수 적용)

# 출력: ŷ = a₂

# 구체적 예시:
# x = 2.0 (입력)
# w = 0.5 (가중치)
# b = 1.0 (편향)
# y = 3.0 (실제 정답)

# 순전파 계산:
# 1. z = w × x + b
#      = 0.5 × 2.0 + 1.0
#      = 2.0

# 2. ŷ = σ(z)  (σ는 활성화 함수, 예: sigmoid)
#      = 1 / (1 + e^(-2.0))
#      ≈ 0.88

# 3. 손실 계산: L = (ŷ - y)²  (예측과 정답의 차이)
#      = (0.88 - 3.0)²
#      = (-2.12)²
#      ≈ 4.49

# 결과: 예측 0.88, 정답 3.0 → 많이 틀렸네요!


# 역전파 (Backpropagation)

# 쉬운 비유:
# 불량품이 나왔을 때 원인 찾기
# [완제품] ← "불량품이네!" (오차 발견)
# [3단계] ← "포장 문제?" (확인)
# [2단계] ← "조립 문제?" (확인)
# [1단계] ← "아! 가공이 문제구나!" (원인 발견)

# 정의:
# 정답과 오차(손실)가 줄어들도록 가중치w, 편향b를 어떻게 바꿀지를
# 미분(체인룰)로 계산해서, 각 층에 '너는 얼마나 책임이 있니?'를 나눠주는 방법

# 왜 '거꾸로' 가냐?
# - 출력 오차는 출력층 파라미터와 가장 직접적으로 연결
# - 그 오차가 그 전 층의 출력 때문에 생긴 부분도 있고
# - 더 전 층 때문에 생긴 부분도 있음
# → 그래서 '오차를 만든 책임'을 출력층에서 시작해 이전 층으로 분배!
# → 이때 쓰는 게 체인 룰(연쇄 법칙)


# 연쇄 법칙 (Chain Rule)
# = 함수가 중첩되어 있을 때 미분하는 방법

# 쉬운 비유:
# 도미노 효과 - 앞의 변화가 뒤에 어떤 영향을 미치는지 계산

# 기본 공식:
# y = f(g(x)) 일 때
# dy/dx = (dy/dg) × (dg/dx)
# 읽는 법: "y를 x로 미분 = y를 g로 미분 × g를 x로 미분"

# 예시: y = (2x + 3)²

# 방법 1: 직접 전개해서 미분
# y = 4x² + 12x + 9
# dy/dx = 8x + 12

# 방법 2: 연쇄 법칙 (중간 변수 사용)
# u = 2x + 3  →  du/dx = 2
# y = u²      →  dy/du = 2u

# 연쇄 법칙 적용:
# dy/dx = (dy/du) × (du/dx)
#       = 2u × 2
#       = 2(2x + 3) × 2
#       = 4(2x + 3)
#       = 8x + 12  ← 같은 결과!

# x = 1일 때:
# dy/dx = 8(1) + 12 = 20

# 왜 연쇄 법칙을 쓰는가?
# 신경망은 함수의 중첩 구조:
# 출력 = f₅(f₄(f₃(f₂(f₁(입력)))))

# 직접 미분? → 너무 복잡!
# 연쇄 법칙? → 가능! (단계별로 곱하면 됨)

# ∂출력/∂입력 = (∂출력/∂f₅) × (∂f₅/∂f₄) × (∂f₄/∂f₃) × (∂f₃/∂f₂) × (∂f₂/∂f₁) × (∂f₁/∂입력)


# 역전파 예시 (가장 단순한 경우)

# 시나리오:
# x → [w곱하기] → z → [제곱] → L

# 수식:
# z = w × x  (w와 x를 곱함)
# L = z²     (z를 제곱함)

# 목표: ∂L/∂w 구하기 (L을 w로 미분 = "w가 L에 얼마나 영향을 주는가?")

# 구체적 숫자:
# x = 3
# w = 2

# 순전파:
# z = w × x = 2 × 3 = 6
# L = z² = 36

# 역전파 (연쇄 법칙 적용):
# ∂L/∂w = (∂L/∂z) × (∂z/∂w)

# 1. ∂L/∂z = 2z = 2 × 6 = 12
#    (L = z²를 z로 미분)

# 2. ∂z/∂w = x = 3
#    (z = w × x를 w로 미분)

# 3. ∂L/∂w = 12 × 3 = 36

# 해석:
# w가 1 증가하면 → L이 약 36 증가
# 따라서 w를 줄여야 L이 감소!


# 경사하강법 (Gradient Descent)
# = 오차를 줄이기 위해 파라미터를 업데이트하는 방법

# 쉬운 비유:
# 안개 낀 산에서 내려오기
# - 눈 가리고 산 정상에 있음
# - 발로 주변 더듬어서 아래 방향 찾기
# - 그 방향으로 한 걸음씩 내려가기

# 공식:
# w_new = w_old - η × (∂L/∂w)
# η (eta) = 학습률 (한 걸음의 크기)

# 예시 (위의 계산 이어서):
# η = 0.01 (학습률)
# w_old = 2
# ∂L/∂w = 36

# w_new = 2 - 0.01 × 36 = 1.64

# 검증 (순전파로 다시 계산):
# z = w × x = 1.64 × 3 = 4.92
# L = z² = 4.92² = 24.21

# 결과: 손실이 36 → 24.21로 감소! ✓


# 학습률 (Learning Rate)의 중요성

# 학습률이 너무 크면 (예: η = 1.0):
# - 한 걸음이 너무 커서 최적점을 지나칠 수 있음
# - 학습이 불안정

# 학습률이 너무 작으면 (예: η = 0.0001):
# - 한 걸음이 너무 작아서 학습이 너무 느림
# - 시간이 오래 걸림

# 적절한 학습률 (예: η = 0.001 ~ 0.01):
# - 안정적으로 빠르게 학습


# 전체 학습 과정 요약

# 1. 순전파: 입력 → 예측값 계산
# 2. 손실 계산: 예측값과 정답의 차이
# 3. 역전파: 각 파라미터가 손실에 미친 영향 계산 (미분)
# 4. 경사하강법: 파라미터 업데이트
# 5. 반복!

# 비유로 기억하기:
# - 순전파 = 공장 생산 라인 (앞으로 진행)
# - 역전파 = 불량품 원인 찾기 (거꾸로 추적)
# - 경사하강법 = 산 내려가기 (오차 줄이기)


print("\n" + "="*50)
print("실습 문제 - 역전파 기초")
print("="*50 + "\n")

# ============================================================
# 실습 1: 단순 선형 함수의 역전파
# ============================================================
print("[ 실습 1: 단순 선형 함수 ]")
print("-" * 50)

# 주어진 값:
# y = 3x + 2
# x = 4일 때, dy/dx를 구하시오

# TODO: 1-1. 손으로 미분 계산
# y = 3x + 2
# dy/dx = ?
manual_derivative = None  # 여기에 답을 적으세요

# TODO: 1-2. 코드로 검증
x = 4.0
y = 3 * x + 2

# 수치 미분 (근사값)
h = 0.0001
y_plus_h = 3 * (x + h) + 2
numerical_derivative = (y_plus_h - y) / h

print(f"손으로 계산한 미분: {manual_derivative}")
print(f"수치 미분 결과: {numerical_derivative:.4f}")
print(f"정답: 3 (상수 미분은 0, x의 계수는 3)\n")


# ============================================================
# 실습 2: 제곱 함수의 역전파
# ============================================================
print("[ 실습 2: 제곱 함수 ]")
print("-" * 50)

# 주어진 값:
# y = x²
# x = 5일 때, dy/dx를 구하시오

# TODO: 2-1. 손으로 미분 계산
# y = x²
# dy/dx = ?
x = 5.0
manual_derivative_2 = None  # 여기에 답을 적으세요 (힌트: 2x)

# TODO: 2-2. 코드로 검증
y = x ** 2

# 수치 미분
h = 0.0001
y_plus_h = (x + h) ** 2
numerical_derivative_2 = (y_plus_h - y) / h

print(f"손으로 계산한 미분: {manual_derivative_2}")
print(f"수치 미분 결과: {numerical_derivative_2:.4f}")
print(f"정답: 2x = 2 × 5 = 10\n")


# ============================================================
# 실습 3: 연쇄 법칙 기초
# ============================================================
print("[ 실습 3: 연쇄 법칙 - 두 단계 함수 ]")
print("-" * 50)

# 주어진 값:
# z = 2x + 1
# y = z²
# x = 3일 때, dy/dx를 구하시오 (연쇄 법칙 사용)

x = 3.0

# TODO: 3-1. 손으로 계산 (연쇄 법칙 적용)
# dy/dx = (dy/dz) × (dz/dx)

# 단계 1: dz/dx = ?
dz_dx = None  # 여기에 답을 적으세요

# 단계 2: dy/dz = ?
z = 2 * x + 1  # z 값을 먼저 계산
dy_dz = None  # 여기에 답을 적으세요 (힌트: y = z²이므로 2z)

# 단계 3: dy/dx = dy/dz × dz/dx
dy_dx = None  # 여기에 답을 적으세요

# TODO: 3-2. 코드로 검증
y = (2 * x + 1) ** 2

# 수치 미분
h = 0.0001
y_plus_h = (2 * (x + h) + 1) ** 2
numerical_derivative_3 = (y_plus_h - y) / h

print(f"dz/dx = {dz_dx}")
print(f"dy/dz = {dy_dz}")
print(f"dy/dx = {dy_dx}")
print(f"수치 미분 결과: {numerical_derivative_3:.4f}")
print(f"정답: 2z × 2 = 2(2×3+1) × 2 = 2×7×2 = 28\n")


# ============================================================
# 실습 4: 손실 함수의 역전파
# ============================================================
print("[ 실습 4: 손실 함수 - 실전 예제 ]")
print("-" * 50)

# 시나리오: 간단한 예측 모델
# 예측: ŷ = w × x
# 손실: L = (ŷ - y)²  (평균제곱오차)
#
# 주어진 값:
# x = 2 (입력)
# y = 7 (정답)
# w = 3 (가중치)
#
# dL/dw를 구하시오 (w를 얼마나 조정해야 할까?)

x = 2.0
y = 7.0
w = 3.0

# TODO: 4-1. 순전파 (값 계산)
y_pred = None  # ŷ = w × x
loss = None    # L = (ŷ - y)²

print(f"순전파:")
print(f"  예측값 ŷ = w × x = {y_pred}")
print(f"  손실 L = (ŷ - y)² = {loss}")

# TODO: 4-2. 역전파 (미분 계산)
# 연쇄 법칙: dL/dw = (dL/dŷ) × (dŷ/dw)

# 단계 1: dŷ/dw = ?
dy_pred_dw = None  # 힌트: ŷ = w × x를 w로 미분

# 단계 2: dL/dŷ = ?
dL_dy_pred = None  # 힌트: L = (ŷ - y)²를 ŷ로 미분 → 2(ŷ - y)

# 단계 3: dL/dw = ?
dL_dw = None  # dL/dŷ × dŷ/dw

print(f"\n역전파:")
print(f"  dŷ/dw = {dy_pred_dw}")
print(f"  dL/dŷ = {dL_dy_pred}")
print(f"  dL/dw = {dL_dw}")

# TODO: 4-3. 경사하강법으로 가중치 업데이트
learning_rate = 0.01
w_new = None  # w_new = w - learning_rate × dL/dw

print(f"\n경사하강법:")
print(f"  학습률 η = {learning_rate}")
print(f"  w_old = {w}")
print(f"  w_new = {w_new}")

# 검증: 새 가중치로 손실이 줄어드는지 확인
if w_new is not None:
    y_pred_new = w_new * x
    loss_new = (y_pred_new - y) ** 2
    print(f"\n검증:")
    print(f"  이전 손실: {loss:.4f}")
    print(f"  새 손실: {loss_new:.4f}")
    print(f"  개선됨: {loss > loss_new}")


# ============================================================
# 실습 5: 도전 과제
# ============================================================
print("\n" + "="*50)
print("[ 실습 5: 도전 과제 ]")
print("="*50)

# 3단계 함수의 역전파
# a = 2x + 1
# b = a²
# c = 3b + 5
# x = 2일 때, dc/dx를 구하시오

x = 2.0

# TODO: 5-1. 순전파로 각 값 계산
a = None  # 2x + 1
b = None  # a²
c = None  # 3b + 5

# TODO: 5-2. 역전파로 미분 계산 (연쇄 법칙 3번 적용!)
# dc/dx = (dc/db) × (db/da) × (da/dx)

da_dx = None  # da/dx
db_da = None  # db/da (힌트: b = a²)
dc_db = None  # dc/db

dc_dx = None  # 최종 답

print(f"순전파: a={a}, b={b}, c={c}")
print(f"역전파: da/dx={da_dx}, db/da={db_da}, dc/db={dc_db}")
print(f"최종: dc/dx = {dc_dx}")

# 수치 미분으로 검증
h = 0.0001
a_h = 2 * (x + h) + 1
b_h = a_h ** 2
c_h = 3 * b_h + 5
numerical = (c_h - c) / h
print(f"수치 미분 검증: {numerical:.4f}")
print(f"정답: dc/db × db/da × da/dx = 3 × 2a × 2 = 6 × 2(2×2+1) = 6 × 10 = 60\n")
