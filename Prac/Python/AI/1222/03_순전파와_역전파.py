# 순전파 (Forward Propagation)
# 입력 -> 출력 방향으로 계산 진행

# 과정
# 1. 입력 데이터 받기
# 2. 각 층에서 가중합 계산: z = Wx + b
# 3. 활성화 함수 적용: a = f(z)
# 4. 다음 층으로 전달
# 5. 출력층까지 반복

# 2층 신경망:
# 층1: z₁ = W₁x + b₁
#      a₁ = f(z₁)

# 층2: z₂ = W₂a₁ + b₂
#      a₂ = f(z₂)

# 출력: ŷ = a₂


# 역전파 (Backpropagation)
# 정답과 오차(손실)가 줄어들도록 가중치w, 편향b를 어떻게 바꿀지를 미분(체인룰)로
# 계산해서, 각 층에 '너는 얼마나 책임이 있니?'를 나눠주는 방법

# 순전파: 입력 -> 층들 통과 -> 예측값

# 왜 '거꾸로' 가냐?
# 출력 오차는 출력층 파라미터와 가장 직접적으로 연결
# 그리고 그 오차가 그 전 층의 출력 때문에 생긴 부분도 있고,
# 더 전 층 때문에 생긴 부분도 있음

# 그래서 '오차를 만든 책임'을 출력층에서 시작해 이전 층으로 분배해야 하는데,
# 이때 쓰는 게 체인 룰입니다.


# 순전파 과정 예시
# 1층 신경망 예시:

# 입력(x) => 가중치 곱하기 -> 편향 더하기 -> 활성화 -> 출력(ŷ)

# 수식
# z = w × x + b
# ŷ = σ(z)  (σ는 활성화 함수, 예: sigmoid)

# 손실
# L = (ŷ - y)²  (y는 실제 값)


# 구체적 예시
# x = 2.0
# w = 0.5
# b = 1.0
# y = 3.0 (실제 타겟)

# 순전파 계산:
# 1. z = w × x + b
#      = 0.5 × 2.0 + 1.0
#      = 2.0

# 2. ŷ = σ(z)
#      = 1 / (1 + e^(-2.0))
#      ≈ 0.88

# 3. L = (ŷ - y)²
#      = (0.88 - 3.0)²
#      = (-2.12)²
#      ≈ 4.49

# 연쇄 법칙(Chain Rule)
# 함수가 중첩되어 있을 때 미분하는 방법

# 예: y = f(g(x))

# 미분:
# dy/dx = (dy/dg) × (dg/dx)

# 읽는 법
# y를 x로 미분 = y를 g로 미분 × g를 x로 미분



# 예: y = (2x + 3)²

# 방법 1: 직접 전개해서 미분
# y = 4x² + 12x + 9
# dy/dx = 8x + 12

# 방법 2: 연쇄 법칙 (중간 변수 사용)
# u = 2x + 3 =>  du/dx = 2
# y = u² =>  dy/du = 2u

# dy/du = 2u
# du/dx = 2

# dy/dx = (dy/du) × (du/dx)
#       = 2u × 2
#       = 2(2x + 3) × 2
#       = 4(2x + 3)
#       = 8x + 12  ← 같은 결과!

# x = 1일 때:
# dy/dx = 8(1) + 12 = 20

# 왜 연쇄 법칙을 쓰는가?
# 신경망은 함수의 중첩 구조:
# 출력 = f₅(f₄(f₃(f₂(f₁(입력)))))

# 직접 미분? -> 너무 복잡
# 연쇄 법칙? -> 가능! (단계별로 곱하면 됨)

# ∂출력/∂입력 = (∂출력/∂f₅) × (∂f₅/∂f₄) × (∂f₄/∂f₃) × (∂f₃/∂f₂) × (∂f₂/∂f₁) × (∂f₁/∂입력)


# 가장 단순한 경우:
# x → [w곱하기] → z → [제곱] → L

# 수식:
# z = w × x
# L = z²

# 목표: ∂L/∂w 구하기


# 예시
# x = 3
# w = 2

# z = w × x = 2 × 3 = 6
# L = z² = 36

# 역전파 (연쇄 법칙):
# ∂L/∂w = (∂L/∂z) × (∂z/∂w)

# 1. ∂L/∂z = 2z = 2 × 6 = 12
#    (L = z²를 z로 미분)

# 2. ∂z/∂w = x = 3
#    (z = w × x를 w로 미분)

# 3. ∂L/∂w = 12 × 3 = 36

# 해석:
# w가 1 증가하면 -> L이 약 36 증가
# w를 줄여야 L이 감소!


# 경사하강법 적용:
# w_new = w_old - η × (∂L/∂w)

# η = 0.01 (학습률)
# w_old = 2
# ∂L/∂w = 36

# w_new = 2 - 0.01 × 36 = 1.64

# 검증 (순전파):
# z = w × x = 1.64 × 3 = 4.92
# L = z² = 4.92² = 24.21

# 손실이 36 -> 24.21로 감소!
